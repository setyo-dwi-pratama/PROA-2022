{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclass_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/setyo-dwi-pratama/PROA-2022/blob/main/multiclass_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Learning -  Multi-class Classification with Imbalanced Data-set"
      ],
      "metadata": {
        "id": "NNKtk2YHHQ0-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WyRw4leOGmiP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import sklearn.datasets as skds\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# For reproducibility\n",
        "np.random.seed(1237)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source file directory\n",
        "# downlaod from http://qwone.com/~jason/20Newsgroups/\n",
        "\n",
        "path_train = \"20news-bydate/20news-bydate-train\"\n",
        "\n",
        "files_train = skds.load_files(path_train,load_content=False)\n",
        "\n",
        "label_index = files_train.target\n",
        "label_names = files_train.target_names\n",
        "labelled_files = files_train.filenames\n",
        "\n",
        "data_tags = [\"filename\",\"category\",\"news\"]\n",
        "data_list = []\n",
        "\n",
        "# Read and add data from file to a list\n",
        "i=0\n",
        "for f in labelled_files:\n",
        "    data_list.append((f,label_names[label_index[i]],Path(f).read_text()))\n",
        "    i += 1\n",
        "\n",
        "# We have training data available as dictionary filename, category, data\n",
        "data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
        "\n",
        "# 20 news groups\n",
        "num_labels = 20\n",
        "vocab_size = 15000\n",
        "batch_size = 100\n",
        "num_epochs = 30\n",
        "\n",
        "# lets take 80% data as training and remaining 20% for test.\n",
        "train_size = int(len(data) * .8)\n",
        "\n",
        "train_posts = data['news'][:train_size]\n",
        "train_tags = data['category'][:train_size]\n",
        "train_files_names = data['filename'][:train_size]\n",
        "\n",
        "test_posts = data['news'][test_size:]\n",
        "test_tags = data['category'][test_size:]\n",
        "test_files_names = data['filename'][test_size:]\n",
        "\n",
        "# define Tokenizer with Vocab Size\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_posts)\n",
        "\n",
        "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
        "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_tags)\n",
        "y_test = encoder.transform(test_tags)"
      ],
      "metadata": {
        "id": "F0d2ujXjHVyC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "95d0d1a4-5414-4e93-b091-c6afba64c611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9d85df6359f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"20news-bydate/20news-bydate-train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfiles_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlabel_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/datasets/_base.py\u001b[0m in \u001b[0;36mload_files\u001b[0;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     folders = [\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     ]\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '20news-bydate/20news-bydate-train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#crate class imbalance; my rough approach\n",
        "data_imb = data.copy()\n",
        "\n",
        "for i in range(1,6):\n",
        "    for index, row in data_imb.iterrows():\n",
        "        if((row[\"category\"] == \"alt.atheism\"   or row[\"category\"] == \"talk.politics.misc\" \n",
        "            or row[\"category\"] ==\"soc.religion.christian\" or row[\"category\"] == \"talk.politics.mideast\")and (index % 3) == 0):\n",
        "            data_imb.drop(index, inplace=True)\n",
        "        elif((row[\"category\"] == \"comp.os.ms-windows.misc\"  or row[\"category\"] == \"comp.sys.ibm.pc.hardware\" or row[\"category\"] == \"comp.graphics\" \n",
        "            or row[\"category\"] ==\"comp.windows.x\" or row[\"category\"] == \"comp.sys.mac.hardware\")and (index % 4) == 0):\n",
        "            data_imb.drop(index, inplace=True)\n",
        "        elif((row[\"category\"] == \"sci.med\"  or row[\"category\"] == \"sci.space\" or row[\"category\"] == \"sci.electronics\" \n",
        "            or row[\"category\"] ==\"sci.crypt\" or row[\"category\"] == \"misc.forsale\") and (index % 5) == 0):\n",
        "            data_imb.drop(index, inplace=True)\n",
        "\n",
        "    data_imb.reset_index(drop = True, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Axq8PHvrHfs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_imb.category.value_counts()"
      ],
      "metadata": {
        "id": "TBj4a_yVHh_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = data_imb.category.tolist()"
      ],
      "metadata": {
        "id": "Vz6mqBoVHksu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_imb.head()"
      ],
      "metadata": {
        "id": "1huPDG-KHmiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 news groups\n",
        "num_labels = 20\n",
        "vocab_size = 15000\n",
        "batch_size = 100\n",
        "num_epochs = 30\n",
        "\n",
        "# lets take 80% data as training and remaining 20% for test.\n",
        "train_size = int(len(data_imb) * .8)\n",
        "\n",
        "train_posts = data_imb['news'][:train_size]\n",
        "train_tags = data_imb['category'][:train_size]\n",
        "train_files_names = data_imb['filename'][:train_size]\n",
        "\n",
        "test_posts = data_imb['news'][test_size:]\n",
        "test_tags = data_imb['category'][test_size:]\n",
        "test_files_names = data_imb['filename'][test_size:]\n",
        "\n",
        "# define Tokenizer with Vocab Size\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_posts)\n",
        "\n",
        "x_train = tokenizer.texts_to_matrix(train_posts)\n",
        "x_test = tokenizer.texts_to_matrix(test_posts)\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_tags)\n",
        "y_test = encoder.transform(test_tags)"
      ],
      "metadata": {
        "id": "CliwMjvNHrVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "U0J_lJhKHtW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let us build a basic model\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(vocab_size,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RFJ0aVG5Ht6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs =10\n",
        "batch_size = 128\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=2,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "id": "gFqVUgCrI22h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=2)\n",
        "\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "id": "3R_WnKJJIys7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#another approach using GRU model, takes longer time\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer_obj = Tokenizer()\n",
        "\n",
        "tokenizer_obj.fit_on_texts(train_posts) \n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_posts])\n",
        "\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "\n",
        "X_train_tokens =  tokenizer_obj.texts_to_sequences(train_posts)\n",
        "X_test_tokens = tokenizer_obj.texts_to_sequences(test_posts)\n",
        "\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_tags)\n",
        "y_test = encoder.transform(test_tags)"
      ],
      "metadata": {
        "id": "gAU8jYqQIvXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#another approach using GRU model, takes longer time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
        "model.add(GRU(units=32,  dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(num_labels, activation='softmax'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print('Summary of the built model...')\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "Y16uRE7GIr4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs =10\n",
        "batch_size = 128\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=2,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "zVVOeOSeHvs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_labels = encoder.classes_\n",
        "\n",
        "for i in range(10):\n",
        "    prediction = model.predict(np.array([x_test[i]]))\n",
        "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
        "    #print(test_files_names.iloc[i])\n",
        "    print('Actual label:' + test_tags.iloc[i])\n",
        "    print(\"Predicted label: \" + predicted_label)\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    # print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "y_pred = model.predict(x_test);\n",
        "cnf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(14, 12, forward=True)\n",
        "#fig.align_labels()\n",
        "\n",
        "# fig.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
        "plot_confusion_matrix(cnf_matrix, classes=np.asarray(label_names), normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "fig.savefig(\"txt_classification-smote\" + str(num_epochs) + \".png\", pad_inches=5.0)"
      ],
      "metadata": {
        "id": "LKYq92_tIn4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(x_test)"
      ],
      "metadata": {
        "id": "-0yHJhC1ImRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = np.argmax(prediction, axis = 1)"
      ],
      "metadata": {
        "id": "OReQN1n-IkpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "IncifwJbIkR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_labels = np.argmax(y_test, axis =1)"
      ],
      "metadata": {
        "id": "leh0B7F1IgUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_labels"
      ],
      "metadata": {
        "id": "kpQO5uuuIb23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_labels = np.argmax(y_train, axis =1)\n",
        "y_train_labels"
      ],
      "metadata": {
        "id": "mqpkadfyIYLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The Kappa score tell you how much better, or worse, your classifier is than what would be expected by random chance. \n",
        "#If you were to randomly assign cases to classes (i.e. a kind of terribly uninformed classifier), you'd get some correct simply by chance. \n",
        "#Therefore, you will always find that the Kappa value is lower than the overall accuracy. \n",
        "#The Kappa index is however considered to be a more conservative measure than the overall classification accuracy. \n",
        "#Your KIA value is telling you essentially that your classifier is about 66% better than a random assignment of cases to the various classes. That's not bad!\n",
        "#A kappa value of 1 represents perfect agreement, while a value of 0 represents no agreement.\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "cohen_score = cohen_kappa_score(y_test_labels, predictions)"
      ],
      "metadata": {
        "id": "rxgNuDRGIVjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cohen_score"
      ],
      "metadata": {
        "id": "vSajMozpIT0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "precision, recall, fscore, support = score(y_test_labels, predictions)\n",
        "\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))\n",
        "print('fscore: {}'.format(fscore))\n",
        "print('support: {}'.format(support))\n",
        "\n"
      ],
      "metadata": {
        "id": "vQ_GFqhiIRuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test_labels, predictions)\n",
        "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
        "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
        "\n",
        "print(recall)\n",
        "\n",
        "print(precision)"
      ],
      "metadata": {
        "id": "XFZZ_NUqIPea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us try some sampling technique to remove class imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "#Over-sampling: SMOTE\n",
        "#SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, \n",
        "#based on those that already exist. It works randomly picking a point from the minority class and computing \n",
        "#the k-nearest neighbors for this point.The synthetic points are added between the chosen point and its neighbors.\n",
        "#We'll use ratio='minority' to resample the minority class.\n",
        "smote = SMOTE('minority')\n",
        "\n",
        "X_sm, y_sm = smote.fit_sample(x_train, y_train)\n",
        "print(X_sm.shape, y_sm.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "uOpFAlPqIMiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weight = class_weight.compute_class_weight('balanced' ,np.unique(y_train_labels) ,y_train_labels)\n",
        "num_epochs =10\n",
        "batch_size = 128\n",
        "history = model.fit(X_sm, y_sm,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=2,\n",
        "                    class_weight=class_weight,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "id": "1lti4lw9IKpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=2)\n",
        "\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "id": "4ulKckyOIDHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(num_labels):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], prediction[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), prediction.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Compute macro-average ROC curve and ROC area\n",
        "\n",
        "# First aggregate all false positive rates\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_labels)]))\n",
        "\n",
        "# Then interpolate all ROC curves at this points\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(num_labels):\n",
        "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# Finally average it and compute AUC\n",
        "mean_tpr /= num_labels\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure()\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "for i, color in zip(range(num_labels), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "CSxgtAZ3H-LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# For each class\n",
        "precision = dict()\n",
        "recall = dict()\n",
        "average_precision = dict()\n",
        "for i in range(num_labels):\n",
        "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
        "                                                        prediction[:, i])\n",
        "    average_precision[i] = average_precision_score(y_test[:, i], prediction[:, i])\n",
        "\n",
        "# A \"micro-average\": quantifying score on all classes jointly\n",
        "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n",
        "    prediction.ravel())\n",
        "average_precision[\"micro\"] = average_precision_score(y_test, prediction,\n",
        "                                                     average=\"micro\")\n",
        "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
        "      .format(average_precision[\"micro\"]))"
      ],
      "metadata": {
        "id": "X4KycDUiH7Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall[\"micro\"], precision[\"micro\"], alpha=0.2, color='b')#,\n",
        "                 #**step_kwargs)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title(\n",
        "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
        "    .format(average_precision[\"micro\"]))"
      ],
      "metadata": {
        "id": "elQKp_jHH4rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import cycle\n",
        "# setup plot details\n",
        "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "f_scores = np.linspace(0.2, 0.8, num=4)\n",
        "lines = []\n",
        "labels = []\n",
        "for f_score in f_scores:\n",
        "    x = np.linspace(0.01, 1)\n",
        "    y = f_score * x / (2 * x - f_score)\n",
        "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
        "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
        "\n",
        "lines.append(l)\n",
        "labels.append('iso-f1 curves')\n",
        "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
        "lines.append(l)\n",
        "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
        "              ''.format(average_precision[\"micro\"]))\n",
        "\n",
        "for i, color in zip(range(num_labels), colors):\n",
        "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
        "    lines.append(l)\n",
        "    labels.append('Precision-recall for class:{0} (area = {1:0.2f})'\n",
        "                  ''.format(i, average_precision[i]))\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.subplots_adjust(bottom=0.25)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall curve to multi-class')\n",
        "plt.legend(lines, labels, loc=(0, -.2), prop=dict(size=10))\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JBDwG6ogH3b9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
